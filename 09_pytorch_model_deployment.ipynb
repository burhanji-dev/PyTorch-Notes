{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94dcbd053bbc43168bedf1514f65b593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89d298564a61487580610ee982efb5e6",
              "IPY_MODEL_0fdf9850c0264caa9ed2746c0cae59cb",
              "IPY_MODEL_1c1a9907ad264a4081efd5f94250430e"
            ],
            "layout": "IPY_MODEL_9fac182de1d347c7afc8e9e1f5b1b854"
          }
        },
        "89d298564a61487580610ee982efb5e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3ad311b22c04c5e9f0866fdf605795f",
            "placeholder": "​",
            "style": "IPY_MODEL_43e5173129964a1b86fbf2974e2b50e7",
            "value": "  0%"
          }
        },
        "0fdf9850c0264caa9ed2746c0cae59cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55a79ac5d308430fa60407a54cff536e",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af52f939bf7647d996b3bfc2eae5bb02",
            "value": 0
          }
        },
        "1c1a9907ad264a4081efd5f94250430e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b9d08d134b749ff9894772f49f28447",
            "placeholder": "​",
            "style": "IPY_MODEL_4dc09fd235ef4764aa062acfdd025922",
            "value": " 0/10 [02:16&lt;?, ?it/s]"
          }
        },
        "9fac182de1d347c7afc8e9e1f5b1b854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ad311b22c04c5e9f0866fdf605795f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43e5173129964a1b86fbf2974e2b50e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55a79ac5d308430fa60407a54cff536e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af52f939bf7647d996b3bfc2eae5bb02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b9d08d134b749ff9894772f49f28447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dc09fd235ef4764aa062acfdd025922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 09. Pytorch Model Deployment\n",
        "\n",
        "What is model deployment?\n",
        "\n",
        "Machine learning model deployment is the act of making your machine learning models availble to someone or something else."
      ],
      "metadata": {
        "id": "64aRR_yDPwlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Get setup"
      ],
      "metadata": {
        "id": "oGYfYi7wQXaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD2zvnK1RAFW",
        "outputId": "4621ff8c-ee39-4667-8904-6a8878bffc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "torch version: 2.2.1+cu121\n",
            "torchvision version: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_batVpCRgsG",
        "outputId": "e9b7586f-15ff-4ee7-f672-e6072023d4ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4056, done.\u001b[K\n",
            "remote: Counting objects: 100% (1234/1234), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 4056 (delta 1141), reused 1124 (delta 1124), pack-reused 2822\u001b[K\n",
            "Receiving objects: 100% (4056/4056), 649.94 MiB | 31.30 MiB/s, done.\n",
            "Resolving deltas: 100% (2386/2386), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "Hsk6elI9QXil",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e28e4529-f3db-4084-f696-932a56b0acf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting data\n",
        "\n",
        "The dataset we're going to use for deploying a FoodVision Mini model is...\n",
        "\n",
        "Pizza, steak, sushi 20% dataset (pizza, steak, sushi classes from Food101, random 20% of the samples)"
      ],
      "metadata": {
        "id": "zHJzv9ugQXme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
        "\n",
        "data_20_percent_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn75Ja6DSozt",
        "outputId": "2d25af03-3ef4-4fd5-9f2b-b99d353970bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Did not find data/pizza_steak_sushi_20_percent directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi_20_percent.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi_20_percent.zip data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi_20_percent')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = data_20_percent_path / \"train\"\n",
        "test_dir = data_20_percent_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPXls9QfSo4S",
        "outputId": "6605046d-27d2-4baa-f51a-a40fba39611e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi_20_percent/train'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. FoodVision Mini model deployment experiment outline\n",
        "\n",
        "### 3 questions\n",
        "1. What is my most ideal machine learning model deployment scenario?\n",
        "2. Where is my model going to go?\n",
        "3. How is my model going to function?\n",
        "\n",
        "**FoodVision Mini ideal use case:** A model that performs well and fast.\n",
        "\n",
        "* Performs well: 95% + accuracy\n",
        "* Fast: as close to real-time (or faster),  as possible (30fps+ or 30ms latency)\n",
        "  * Latency = time for predictions to take place\n",
        "\n",
        "To try and achieve these goals we are going to build 2 model experiments:\n",
        "\n",
        "1. EffNetB2 feature extractor (just like in 07. Pytorch Experiment tracking)\n",
        "2. ViT feature extractor (just like in 08. Pytorch paper replicating)"
      ],
      "metadata": {
        "id": "laj4OCxmQXu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Creating an EffnetB2 feature extractor\n",
        "\n",
        "Feature extractor = a term for a transfer learning model that has its base layers froze and the output layers (or head/classifier) customized to fit a specific problem"
      ],
      "metadata": {
        "id": "zDTWq-XrXwA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# 1. Setup pretrained EffNetB2 weights\n",
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # DEFAULT is = best available\n",
        "\n",
        "# 2. Get EffNetB2 transforms\n",
        "effnetb2_transforms = effnetb2_weights.transforms()\n",
        "\n",
        "# 3. Setup a pretrained model instance\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # Could alse use weights = \"DEFAULT\"\n",
        "\n",
        "# 4. freeze the base layers in the model (will stop all layers from training)\n",
        "for param in effnetb2.parameters():\n",
        "  param.requires_grad = False\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvarZV0WX2UM",
        "outputId": "7a1b2c63-f5f7-47bf-8ec5-ecd28ae9b637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n",
            "100%|██████████| 35.2M/35.2M [00:00<00:00, 76.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1dJD-OwaURW",
        "outputId": "98846e9d-032e-4e54-966b-76d975304eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU(inplace=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
              "      )\n",
              "      (1): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
              "      )\n",
              "      (1): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
              "      )\n",
              "      (2): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
              "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
              "      )\n",
              "      (1): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
              "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
              "      )\n",
              "      (2): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
              "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
              "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
              "      )\n",
              "      (1): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
              "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
              "      )\n",
              "      (2): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
              "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
              "      )\n",
              "      (3): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
              "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(528, 528, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=528, bias=False)\n",
              "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(528, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
              "      )\n",
              "      (1): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
              "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
              "      )\n",
              "      (2): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
              "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
              "      )\n",
              "      (3): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
              "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=720, bias=False)\n",
              "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(720, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
              "      )\n",
              "      (1): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
              "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
              "      )\n",
              "      (2): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
              "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
              "      )\n",
              "      (3): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
              "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
              "      )\n",
              "      (4): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
              "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1248, bias=False)\n",
              "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(1248, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
              "      )\n",
              "      (1): MBConv(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(352, 2112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(2112, 2112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2112, bias=False)\n",
              "            (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): SiLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(2112, 88, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(88, 2112, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): SiLU(inplace=True)\n",
              "            (scale_activation): Sigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(2112, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (8): Conv2dNormActivation(\n",
              "      (0): Conv2d(352, 1408, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.3, inplace=True)\n",
              "    (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print EffNetB2 model summary (uncomment for full output)\n",
        "summary(effnetb2,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-4jcjPOahlo",
        "outputId": "98758e98-ea1d-415c-b599-7ca34798e29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 1000]            --                   False\n",
              "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
              "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
              "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
              "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
              "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
              "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
              "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
              "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
              "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
              "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
              "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
              "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
              "├─Sequential (classifier)                                    [1, 1408]            [1, 1000]            --                   False\n",
              "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
              "│    └─Linear (1)                                            [1, 1408]            [1, 1000]            (1,409,000)          False\n",
              "============================================================================================================================================\n",
              "Total params: 9,109,994\n",
              "Trainable params: 0\n",
              "Non-trainable params: 9,109,994\n",
              "Total mult-adds (M): 659.05\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.81\n",
              "Params size (MB): 36.44\n",
              "Estimated Total Size (MB): 193.85\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seeds for reproducibility\n",
        "set_seeds()\n",
        "\n",
        "effnetb2.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3, inplace=True),\n",
        "    nn.Linear(in_features=1408, out_features=3, bias=True)\n",
        ")"
      ],
      "metadata": {
        "id": "B5Dzl-G8ahhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print EffNetB2 model summary (uncomment for full output)\n",
        "summary(effnetb2,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBnu_-itahdF",
        "outputId": "ed2816b8-4399-43ee-82b9-3f6ae7b2a606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 3]               --                   Partial\n",
              "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
              "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
              "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
              "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
              "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
              "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
              "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
              "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
              "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
              "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
              "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
              "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
              "├─Sequential (classifier)                                    [1, 1408]            [1, 3]               --                   True\n",
              "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
              "│    └─Linear (1)                                            [1, 1408]            [1, 3]               4,227                True\n",
              "============================================================================================================================================\n",
              "Total params: 7,705,221\n",
              "Trainable params: 4,227\n",
              "Non-trainable params: 7,700,994\n",
              "Total mult-adds (M): 657.64\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.80\n",
              "Params size (MB): 30.82\n",
              "Estimated Total Size (MB): 188.22\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Create a function to make an EffNetB2 feature extractor"
      ],
      "metadata": {
        "id": "mvolnsx3ahIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb2_model(num_classes:int=3, # Default put classes: [pizza, steak, sushi]\n",
        "                          seed:int=42):\n",
        "  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "  transforms = weights.transforms()\n",
        "\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # 4. Freeze all the base layers\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # 5. change classifier head with random seed for reproducibilty\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.3, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "id": "tnBkAb6-cHa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2, effnetb2_transforms = create_effnetb2_model()"
      ],
      "metadata": {
        "id": "5KfmumkTb9K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_transforms, effnetb2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjFnZQzBb9Uu",
        "outputId": "f9ac8af3-90bb-4920-f1d7-97b85d510dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ImageClassification(\n",
              "     crop_size=[288]\n",
              "     resize_size=[288]\n",
              "     mean=[0.485, 0.456, 0.406]\n",
              "     std=[0.229, 0.224, 0.225]\n",
              "     interpolation=InterpolationMode.BICUBIC\n",
              " ),\n",
              " EfficientNet(\n",
              "   (features): Sequential(\n",
              "     (0): Conv2dNormActivation(\n",
              "       (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (2): SiLU(inplace=True)\n",
              "     )\n",
              "     (1): Sequential(\n",
              "       (0): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "             (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (2): Conv2dNormActivation(\n",
              "             (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
              "       )\n",
              "       (1): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "             (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (2): Conv2dNormActivation(\n",
              "             (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
              "       )\n",
              "     )\n",
              "     (2): Sequential(\n",
              "       (0): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "             (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
              "       )\n",
              "       (1): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "             (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
              "       )\n",
              "       (2): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "             (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
              "       )\n",
              "     )\n",
              "     (3): Sequential(\n",
              "       (0): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
              "             (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
              "       )\n",
              "       (1): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
              "             (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
              "       )\n",
              "       (2): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
              "             (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
              "       )\n",
              "     )\n",
              "     (4): Sequential(\n",
              "       (0): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
              "             (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
              "       )\n",
              "       (1): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
              "             (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
              "       )\n",
              "       (2): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
              "             (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
              "       )\n",
              "       (3): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
              "             (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
              "       )\n",
              "     )\n",
              "     (5): Sequential(\n",
              "       (0): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(528, 528, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=528, bias=False)\n",
              "             (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(528, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
              "       )\n",
              "       (1): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
              "             (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
              "       )\n",
              "       (2): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
              "             (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
              "       )\n",
              "       (3): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
              "             (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
              "       )\n",
              "     )\n",
              "     (6): Sequential(\n",
              "       (0): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=720, bias=False)\n",
              "             (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(720, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
              "       )\n",
              "       (1): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
              "             (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
              "       )\n",
              "       (2): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
              "             (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
              "       )\n",
              "       (3): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
              "             (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
              "       )\n",
              "       (4): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
              "             (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
              "       )\n",
              "     )\n",
              "     (7): Sequential(\n",
              "       (0): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1248, bias=False)\n",
              "             (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(1248, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
              "       )\n",
              "       (1): MBConv(\n",
              "         (block): Sequential(\n",
              "           (0): Conv2dNormActivation(\n",
              "             (0): Conv2d(352, 2112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (1): Conv2dNormActivation(\n",
              "             (0): Conv2d(2112, 2112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2112, bias=False)\n",
              "             (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "             (2): SiLU(inplace=True)\n",
              "           )\n",
              "           (2): SqueezeExcitation(\n",
              "             (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "             (fc1): Conv2d(2112, 88, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (fc2): Conv2d(88, 2112, kernel_size=(1, 1), stride=(1, 1))\n",
              "             (activation): SiLU(inplace=True)\n",
              "             (scale_activation): Sigmoid()\n",
              "           )\n",
              "           (3): Conv2dNormActivation(\n",
              "             (0): Conv2d(2112, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "             (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "           )\n",
              "         )\n",
              "         (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
              "       )\n",
              "     )\n",
              "     (8): Conv2dNormActivation(\n",
              "       (0): Conv2d(352, 1408, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (2): SiLU(inplace=True)\n",
              "     )\n",
              "   )\n",
              "   (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "   (classifier): Sequential(\n",
              "     (0): Dropout(p=0.3, inplace=True)\n",
              "     (1): Linear(in_features=1408, out_features=3, bias=True)\n",
              "   )\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print EffNetB2 model summary (uncomment for full output)\n",
        "summary(effnetb2,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EorQgikb9be",
        "outputId": "1f3c7378-ca51-491b-8d39-4b347bd3f7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 3]               --                   Partial\n",
              "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
              "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
              "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
              "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
              "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
              "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
              "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
              "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
              "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
              "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
              "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
              "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
              "├─Sequential (classifier)                                    [1, 1408]            [1, 3]               --                   True\n",
              "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
              "│    └─Linear (1)                                            [1, 1408]            [1, 3]               4,227                True\n",
              "============================================================================================================================================\n",
              "Total params: 7,705,221\n",
              "Trainable params: 4,227\n",
              "Non-trainable params: 7,700,994\n",
              "Total mult-adds (M): 657.64\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.80\n",
              "Params size (MB): 30.82\n",
              "Estimated Total Size (MB): 188.22\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=effnetb2_transforms,\n",
        "                                                                               batch_size=32)\n",
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvdc8gsIQXzx",
        "outputId": "498db085-9885-4e19-d4ce-d379a46bc2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7a46b4ac1b70>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7a46b4ac1b40>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Training EffNetB2 feature extractor"
      ],
      "metadata": {
        "id": "0qR5ZoLJemNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "# training function (engine.py)\n",
        "set_seeds()\n",
        "effnetb2_results = engine.train(model=effnetb2,\n",
        "                                train_dataloader=train_dataloader_effnetb2,\n",
        "                                test_dataloader=test_dataloader_effnetb2,\n",
        "                                optimizer=optimizer,\n",
        "                                loss_fn=loss_fn,\n",
        "                                epochs=10,\n",
        "                                device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382,
          "referenced_widgets": [
            "94dcbd053bbc43168bedf1514f65b593",
            "89d298564a61487580610ee982efb5e6",
            "0fdf9850c0264caa9ed2746c0cae59cb",
            "1c1a9907ad264a4081efd5f94250430e",
            "9fac182de1d347c7afc8e9e1f5b1b854",
            "e3ad311b22c04c5e9f0866fdf605795f",
            "43e5173129964a1b86fbf2974e2b50e7",
            "55a79ac5d308430fa60407a54cff536e",
            "af52f939bf7647d996b3bfc2eae5bb02",
            "7b9d08d134b749ff9894772f49f28447",
            "4dc09fd235ef4764aa062acfdd025922"
          ]
        },
        "id": "3BUVI9NUsfq_",
        "outputId": "74de7a47-4203-419d-84bd-ab3f2a883d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94dcbd053bbc43168bedf1514f65b593"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-70b32645390f>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# training function (engine.py)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mset_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m effnetb2_results = engine.train(model=effnetb2,\n\u001b[0m\u001b[1;32m     13\u001b[0m                                 \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader_effnetb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                 \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader_effnetb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/going_modular/going_modular/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# Loop through training and testing steps for a number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         train_loss, train_acc = train_step(model=model,\n\u001b[0m\u001b[1;32m    170\u001b[0m                                           \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                                           \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/going_modular/going_modular/engine.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# 1. Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# 2. Calculate  and accumulate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Plot the loss curves"
      ],
      "metadata": {
        "id": "YVdbAyeot05_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(effnetb2_results)"
      ],
      "metadata": {
        "id": "Eb4f50xouY1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Saving EffNetB2 feature extractor to file\n"
      ],
      "metadata": {
        "id": "BMBWYdB9uY5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# Save the model\n",
        "utils.save_model(model=effnetb2,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "_3QPUSHKvYqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Inspecting the size of our EffNetB2 feature extractor\n",
        "\n",
        "Why would it be important to consider the size of a saved model?\n",
        "\n",
        "If we're deploying our model to be used on a mobile app/webstie, there may be limited compute resources.\n",
        "\n",
        "so if our model file is too large,  we may not be able to store/run it on our target device\n"
      ],
      "metadata": {
        "id": "DfubuEWnuZBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size / (1024*1024)\n",
        "print(f\" Pretrained EffnetB2 feature extractor model size: {round(pretrained_effnetb2_model_size, 2)} MB\")"
      ],
      "metadata": {
        "id": "eIYOBNYwwf72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Collecting EffnetB2 feature extractor stats"
      ],
      "metadata": {
        "id": "ZLKmapeYwf_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in EffNetB2\n",
        "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
        "effnetb2_total_params\n"
      ],
      "metadata": {
        "id": "1kqhGwJxJbtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with effnetb2 statistics\n",
        "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
        "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
        "                  \"number_of_params\": effnetb2_total_params,\n",
        "                  \"model_size_(MB)\": pretrained_effnetb2_model_size}\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "XAJaMW2hwgCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Creating a ViT feature extractor\n",
        "\n",
        "We're up to our second modeling experiment, repeating the steps for EffNetB2 but this time with a ViT feature extractor"
      ],
      "metadata": {
        "id": "i1pGnm6Gt8GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the vision transformer heads layer\n",
        "vit = torchvision.models.vit_b_16(weights=\"DEFAULT\")\n",
        "vit.heads"
      ],
      "metadata": {
        "id": "Y4d_Y34VMO3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_model(num_classes:int=3,\n",
        "                     seed:int=42):\n",
        "  # Create a Vit_B_16 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.vit_b_16(weights=weights)\n",
        "\n",
        "  # Freeze all of the base layers\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Change the classifier head to suit our needs\n",
        "  torch.manual_seed(seed)\n",
        "  model.heads = nn.Sequential(\n",
        "      nn.Linear(in_features=768, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms\n"
      ],
      "metadata": {
        "id": "FahEZaiYMO7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit, vit_transforms = create_vit_model()\n",
        "vit_transforms"
      ],
      "metadata": {
        "id": "Yz8nimnXMPJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print EffNetB2 model summary (uncomment for full output)\n",
        "summary(vit,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "6HVKjcGQMPPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Creating dataloaders"
      ],
      "metadata": {
        "id": "kJI0dJ6cPkfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                       test_dir=test_dir,\n",
        "                                                                                       transform=vit_transforms,\n",
        "                                                                                       batch_size=32)\n",
        "\n",
        "len(train_dataloader_vit), len(test_dataloader_vit), class_names"
      ],
      "metadata": {
        "id": "I1nexhUmMPgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=0.001)\n",
        "\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train ViT feature extractor with seeds set for reproducibility\n",
        "set_seeds()\n",
        "\n",
        "vit_results = engine.train(model=vit,\n",
        "                           train_dataloader=train_dataloader_vit,\n",
        "                           test_dataloader=test_dataloader_vit,\n",
        "                           optimizer=optimizer,\n",
        "                           loss_fn=loss_fn,\n",
        "                           epochs=10,\n",
        "                           device=device)"
      ],
      "metadata": {
        "id": "kDWtEfNeMPnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Plot the loss curves of ViT feature extractor"
      ],
      "metadata": {
        "id": "EyfSeILzQpc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(vit_results)"
      ],
      "metadata": {
        "id": "fNzXD9IHRLtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Saving ViT Feature extractor"
      ],
      "metadata": {
        "id": "ftGkqfnzRLxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=vit,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "Fb50hDlkR4DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size / (1024 * 1024)\n",
        "print(f\"pretrained Vit feature extractor model size: {round(pretrained_vit_model_size, 2)} MB\")"
      ],
      "metadata": {
        "id": "MGkdkjKQR4NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
        "vit_total_params"
      ],
      "metadata": {
        "id": "uUmlCuL0ThEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with effnetb2 statistics\n",
        "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
        "                  \"test_acc\": vit_results[\"test_acc\"][-1],\n",
        "                  \"number_of_params\": vit_total_params,\n",
        "                  \"model_size_(MB)\": pretrained_vit_model_size}\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "QnC6hy7eR4TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Making predictions with our trained models and time them\n",
        "\n",
        "Our goal:\n",
        "1. performs well (95%+ test accuracy)\n",
        "2. fast (30+fps)\n",
        "\n",
        "To test criteria two:\n",
        "1. loop through test images\n",
        "2. Time how long each model takes to make a prediction on the image\n",
        "\n",
        "Let's work towards making a function called `pred_and_store()` to do so.\n",
        "\n",
        "First we will need a list of test image paths."
      ],
      "metadata": {
        "id": "WUj2ht9MR4YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get all test data Paths\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[:5]"
      ],
      "metadata": {
        "id": "XG5-A9PGRL1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Creating a function to make across the test dataset\n",
        "\n",
        "1. Create a function that takes a list of paths and a trained pytorch model and a series of transforms, a list of target class names and a target device\n",
        "2. Create an empty lis (can return a full list of all predictions later).\n",
        "3. Loop through the target input paths (the rest of the steps will take place inside of the loop).\n",
        "4. Create an empty dictionary for each sample (predictions statistics will go in here)\n",
        "5. Get the sample path and ground truth class from the filepath.\n",
        "6. Start the prediction timer.\n",
        "7. Open the image using `PIL.Image.open(path)`.\n",
        "8. Transform the image to be usable with a given model\n",
        "9. Prepare the model for inference by sending it to the target device and turning on eval mode.\n",
        "10. Turn on torch inference mode and pass the target transformed image to the model and perform forward pass + calculate pre prob + pred class.\n",
        "11. Add the pred prob + pred class to empty dictionary from step 4.\n",
        "12. end the prediction timer started in step 6 and add the time to the prediction dictionary.\n",
        "13. See if the predicted class matches the ground truth class.\n",
        "14. Append the updated prediction dictionary to the empty list of predictions we created in step 2.\n",
        "15. return the list of prediction dictionaries"
      ],
      "metadata": {
        "id": "MAysvSTFVoY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import I\n",
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict\n",
        "\n",
        "# 1.Create a function that takes a list of paths and a trained pytorch model and a series of transforms, a list of target class names and a target device\n",
        "def pred_and_store(paths: List[pathlib.Path],\n",
        "                   model: torch.nn.Module,\n",
        "                   transform: torchvision.transforms,\n",
        "                   class_names: List[str],\n",
        "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
        "  # 2. Create an empty lis (can return a full list of all predictions later).\n",
        "  pred_list = []\n",
        "\n",
        "  # 3. Loop through the target input paths (the rest of the steps will take place inside of the loop).\n",
        "  for path in tqdm(paths):\n",
        "\n",
        "    # 4. Create an empty dictionary for each sample (predictions statistics will go in here)\n",
        "    pred_dict = {}\n",
        "\n",
        "    # 5. Get the sample path and ground truth class from the filepath.\n",
        "    pred_dict[\"image_path\"] = path\n",
        "    class_name = path.parent.stem\n",
        "    pred_dict[\"class_name\"] = class_name\n",
        "\n",
        "    # 6. Start the prediction timer.\n",
        "    start_time = timer()\n",
        "\n",
        "    # 7. Open the image using PIL.Image.open(path).\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # 8. Transform the image to be usable with a given model (also add a batch dimension, and send it to the target device)\n",
        "    transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # 9. Prepare the model for inference by sending it to the target device and turning on eval mode\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 10. Turn on torch inference mode and pass the target transformed image to the model and perform forward pass + calculate pre prob + pred class.\n",
        "    with torch.inference_mode():\n",
        "      pred_logit = model(transformed_image)\n",
        "      pred_prob = torch.softmax(pred_logit, dim=1) # Turn logit into prediction probabilities\n",
        "      pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probaility into prediction label\n",
        "      pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on the cpu (python variables live on the cpu)\n",
        "\n",
        "      # 11. Add the pred prob + pred class to empty dictionary from step 4.\n",
        "      pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
        "      pred_dict[\"pred_class\"] = pred_class\n",
        "\n",
        "      # 12. end the prediction timer started in step 6 and add the time to the prediction dictionary.\n",
        "      end_time = timer()\n",
        "      pred_dict[\"time_for_pred\"] = round(end_time - start_time, 4)\n",
        "\n",
        "    # 13. See if the predicted class matches the ground truth class\n",
        "    pred_dict[\"correct\"] = class_name == pred_class\n",
        "\n",
        "    # 14. Append the updated prediction dictionary to the empty list of predictions we created in step 2\n",
        "    pred_list.append(pred_dict)\n",
        "\n",
        "  # 15.return the list of prediction dictionaries\n",
        "  return pred_list\n"
      ],
      "metadata": {
        "id": "HubBb3sfW_WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Making and timing predictions with EffNetB2\n",
        "\n",
        "Let's test our `pred_and_store()` function\n",
        "\n",
        "Two things to note:\n",
        "1. Device - we're going to hardcode our predictions to happen on CPU (because you wont always be sure of having a GPU when you deploy your model).\n",
        "2. Transforms - we want to make sure each of the models are prediction on images that have been prepared with the appropriate transforms (e.g. EffNetB2 with `effnetb2_transforms`)"
      ],
      "metadata": {
        "id": "ViAgpQP2W_ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test dataset with EffNetB2\n",
        "effnetB2_test_pred_dict = pred_and_store(paths=test_data_paths,\n",
        "                                         model=effnetb2,\n",
        "                                         transform=effnetb2_transforms,\n",
        "                                         class_names=class_names,\n",
        "                                         device=\"cpu\") # Hardcode predictions to happen on CPU"
      ],
      "metadata": {
        "id": "ffnDitJOW_eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetB2_test_pred_dict[:2]"
      ],
      "metadata": {
        "id": "MhMwWVDAW_hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "effnetb2_test_pred_df = pd.DataFrame(effnetB2_test_pred_dict)\n",
        "effnetb2_test_pred_df.head()"
      ],
      "metadata": {
        "id": "M3PIlnb1WRp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_test_pred_df[\"correct\"].value_counts()"
      ],
      "metadata": {
        "id": "CtLTnSWmrEeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
        "effnetb2_average_time_per_pred"
      ],
      "metadata": {
        "id": "Mo4bekBhrdE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "z5w6K5bOvY6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Making and timing predictions with VIT"
      ],
      "metadata": {
        "id": "7kumthjvvP0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test dataset with vit\n",
        "vit_test_pred_dict = pred_and_store(paths=test_data_paths,\n",
        "                                         model=vit,\n",
        "                                         transform=vit_transforms,\n",
        "                                         class_names=class_names,\n",
        "                                         device=\"cpu\") # Hardcode predictions to happen on CPU"
      ],
      "metadata": {
        "id": "Q4JED11GsDit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_test_pred_dict[:2]"
      ],
      "metadata": {
        "id": "__YAW3RpsDo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn vit_test_pred_dicts into a dataframe\n",
        "import pandas as pd\n",
        "vit_test_pred_df = pd.DataFrame(vit_test_pred_dict)\n",
        "vit_test_pred_df.head()"
      ],
      "metadata": {
        "id": "EcBIF0u9sDxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_test_pred_df[\"correct\"].value_counts()"
      ],
      "metadata": {
        "id": "d3qMVTpFsD4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average time for prediction for VIT model\n",
        "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
        "vit_average_time_per_pred"
      ],
      "metadata": {
        "id": "b6DXGSPgt23q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add average time per prediction to VIT stats\n",
        "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "zryRW_1Qt-3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Comparing model results, prediction times and size"
      ],
      "metadata": {
        "id": "MbbNmeDfvHpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn stat dictionaries into a Dataframe\n",
        "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
        "\n",
        "# Add column for model names\n",
        "df[\"Model\"] = [\"EffNetB2\", \"ViT\"]\n",
        "\n",
        "# convert accuracy to percentages\n",
        "df[\"test_acc\"] = round(df.test_acc * 100, 2)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "IIFoVOmbv8u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which model is better?\n",
        "\n",
        "* `test_loss` (lower is better) - ViT\n",
        "* `test_acc` (higher is better) - ViT\n",
        "* `number_of_patams` (generally lower is better) - EffNetB2, if a model has more parameters it generally will take longer to compute\n",
        "  * *somtimes models with higher parameters can still perform fast\n",
        "* `model_size_(MB)` - EffNetB2 (for our use case of deploying to a mobile device, generally lower is better)\n",
        "* `time_per_pred_cpu` - (lower is better,  will be highly dependant on the hardware youre running on) - EffNetB2\n",
        "\n",
        "Both models fail to achieve our goal of 30+fps... however we could always try and use EffNetB2 and see how it goes"
      ],
      "metadata": {
        "id": "BLkbv2RSv8zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare ViT to EffNetB2 across different characteristics\n",
        "pd.DataFrame(df.set_index(\"Model\").loc[\"ViT\"] / df.set_index(\"Model\").loc[\"EffNetB2\"],\n",
        "             columns=[\"ViT to EffNetB2 ratios\"]).T"
      ],
      "metadata": {
        "id": "lfnuztY8v83p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Visualizing the speed vs performance tradeoff\n",
        "\n",
        "So we've compared our EffNetB2 and ViT feature extractor models, now lets visualize the comparison with a speed vs performance plot.\n",
        "\n",
        "We can do so with matplotlib:\n",
        "1. Create a scatter plot from the comparison DataFrame to compare EffNetB2 and ViT across test accuracy and prediction time.\n",
        "2. Add titles and labels to make our plot look nice\n",
        "3. Annotat the samples on the scatter plot so we know whats going on.\n",
        "4. Create a legend based on the model sizes (`model_size (MB)`)"
      ],
      "metadata": {
        "id": "8HdqavXMv88j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a plot from model comparison DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "scatter = ax.scatter(data=df,\n",
        "                     x=\"time_per_pred_cpu\",\n",
        "                     y=\"test_acc\",\n",
        "                     c=[\"blue\", \"orange\"],\n",
        "                     s=\"model_size_(MB)\")\n",
        "\n",
        "# 2. Add titles and labels to make our plot look good\n",
        "ax.set_title(\"FoodVision Mini Inference Speed vs Performace\", fontsize=18)\n",
        "ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\n",
        "ax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\n",
        "ax.tick_params(axis=\"both\", labelsize=12)\n",
        "ax.grid(True)\n",
        "\n",
        "# 3. Annotate the samples on scatter plot so we know whats going on\n",
        "for index, row in df.iterrows():\n",
        "  ax.annotate(text=row[\"Model\"],\n",
        "              xy=(row[\"time_per_pred_cpu\"] + 0.0006, row[\"test_acc\"] + 0.04),\n",
        "              size=12)\n",
        "\n",
        "# 4. Create a legend based on the model sizes (model_size (MB))\n",
        "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
        "model_size_legend = ax.legend(handles,\n",
        "                              labels,\n",
        "                              loc=\"lower right\",\n",
        "                              title=\"Model_size_(MB)\",\n",
        "                              fontsize=12)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(\"09-foodvision-mini-inference-speed-vs-performance.png\")"
      ],
      "metadata": {
        "id": "5AdwKvNaPx8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Bringing Foodvision Mini to life by creating a Gradio demo\n",
        "\n",
        "We've chosen to deploy EffNetB2 as it fulfills our criteria the best.\n",
        "\n",
        "What is gradio?\n",
        "\n",
        "> Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!"
      ],
      "metadata": {
        "id": "IgeP5hXZPyAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import gradio as gr\n",
        "  print(\"found gradio, importing it...\")\n",
        "except:\n",
        "  print(\"Couldn't find gradio, installing it...\")\n",
        "  !pip -q install gradio\n",
        "  import gradio as gr"
      ],
      "metadata": {
        "id": "yx10CdyuXNxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.__version__"
      ],
      "metadata": {
        "id": "OPhwx1sLXMYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Gradio overview\n",
        "\n",
        "Gradio helps you create machine learning demos.\n",
        "\n",
        "Why create a demo?\n",
        "\n",
        "So other people can try our models and we can test them in the real-world\n",
        "\n",
        "Deployment is as important as training\n",
        "\n",
        "The overall premise of Gradio is to map inputs -> function/model -> outputs"
      ],
      "metadata": {
        "id": "NwTYR-y4ZEXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Creating a function to map our inputs and outputs (what we are feeding to the gradio interface class)\n"
      ],
      "metadata": {
        "id": "u44g7SvEXLz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Put our model on the cpu\n",
        "effnetb2 = effnetb2.to(\"cpu\") # gradio will run on cpu\n",
        "\n",
        "# Check the device\n",
        "next(iter(effnetb2.parameters())).device"
      ],
      "metadata": {
        "id": "u55Ie5SfXCh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's Create a function called `predict()` to go from:\n",
        "\n",
        "```\n",
        "images of food -> ML model (EffNetB2) -> outputs (food label)\n",
        "```"
      ],
      "metadata": {
        "id": "-XNq96s0aRUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Dict\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  # Start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Transform the input image for use with EffNetB2\n",
        "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on the 0th dimension\n",
        "\n",
        "  # Put model into eval mode, make prediction\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass transformed image through the model and turn the prediction logits into probabilities\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # Calculate the pred time\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  return pred_labels_and_probs, pred_time"
      ],
      "metadata": {
        "id": "sEdBHXIpaaq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Get a list of all test image filepaths\n",
        "\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "\n",
        "# Randomly select a test image path\n",
        "random_image_path = random.sample(test_data_paths, k=1)[0] # [0] gets the path itself\n",
        "random_image_path\n",
        "\n",
        "# Open the target image\n",
        "image = Image.open(random_image_path)\n",
        "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
        "\n",
        "# Predict on the target image and print out the outputs\n",
        "pred_dict, pred_time = predict(image)\n",
        "print(pred_dict)\n",
        "print(pred_time)"
      ],
      "metadata": {
        "id": "Yjifeb7eaa1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Creating a List of example images"
      ],
      "metadata": {
        "id": "Ee-vvolIaa6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of example inputs to our gradio demo\n",
        "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]"
      ],
      "metadata": {
        "id": "YQmEMz7xfxlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4  BUilding a gradio interface\n",
        "\n",
        "Let's use `gr.Interface` to go from:\n",
        "\n",
        "```\n",
        "input: image -> transform -> predict with effnetb2 - > output: pred, pred prob, time\n",
        "```"
      ],
      "metadata": {
        "id": "-N5b0ehSfzTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description, and article\n",
        "\n",
        "title = \"FoodVision Mini🍕🥩🍥\"\n",
        "description = \"An [EfficientNetB2 feature extractor](https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2) computer vision model to classify images as pizza, steak, or sushi\"\n",
        "article = \"Created at [09. Pytorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/#1-getting-data)\"\n",
        "\n",
        "# Create the gradio demo\n",
        "demo = gr.Interface(fn=predict, # Maps our inputs to outputs\n",
        "                    inputs=gr.Image(type=\"pil\"),\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"predictions\"),\n",
        "                             gr.Number(label=\"Prediction time (s)\")],\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL\n"
      ],
      "metadata": {
        "id": "cX6BN9xRfzXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Turning our FoodVision Mini Gradio Demo into a deployable app\n",
        "\n",
        "Our gradio demos from google colab are fantastic,  but they expire after 72 hours\n",
        "\n",
        "To fix this, we're going to prepare our app files so we can host them on Hugging Face spaces: https://huggingface.co/docs/hub/spaces"
      ],
      "metadata": {
        "id": "aIe0niZAfzbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 What is hugging face spaces?\n",
        "\n",
        "Hugging Face Spaces offer a simple way to host ML demo apps directly on your profile or your organization’s profile. This allows you to create your ML portfolio, showcase your projects at conferences or to stakeholders, and work collaboratively with other people in the ML ecosystem\n",
        "\n",
        "If GitHub is a place to show your coding ability, Hugging face spaces is a place to show your machine learning ability (through sharing ML demos that you've built)"
      ],
      "metadata": {
        "id": "JwRXuSPVxQDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Deployed gradio app structure\n",
        "\n",
        "Let's start to put all of our app files into a single directory:\n",
        "\n",
        "```\n",
        "Colab -> folder with all gradio files -> upload app files to hugging face spaces -> deploy\n",
        "```\n",
        "\n",
        "By the end our file structure will look like:\n",
        "```\n",
        "demos/\n",
        "└── foodvision_mini/\n",
        "    ├── 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
        "    ├── app.py\n",
        "    ├── examples/\n",
        "    │   ├── example_1.jpg\n",
        "    │   ├── example_2.jpg\n",
        "    │   └── example_3.jpg\n",
        "    ├── model.py\n",
        "    └── requirements.txt\n",
        "```\n",
        "\n",
        "Why use this structure?\n",
        " because its one of the simplest we could start with."
      ],
      "metadata": {
        "id": "zxDpM2I9fzlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Creating a `deoms` folder to store our foodvision mini app files"
      ],
      "metadata": {
        "id": "CYgatt7wfxo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create Foodvision mini demo path\n",
        "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
        "\n",
        "# Remove files that might exists and create a new directory\n",
        "if foodvision_mini_demo_path.exists():\n",
        "  shutil.rmtree(foodvision_mini_demo_path)\n",
        "  foodvision_mini_demo_path.mkdir(parents=True,\n",
        "                                  exist_ok=True)\n",
        "else:\n",
        "  foodvision_mini_demo_path.mkdir(parents=True,\n",
        "                                  exist_ok=True)\n",
        "\n",
        "!ls demos/foodvision_mini/"
      ],
      "metadata": {
        "id": "fkIPBaDudjf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Creating a folder of example images to use with our foodvision mini demo\n",
        "\n",
        "what we want:\n",
        "* 3 images in an `examples/` directory\n",
        "* images should be from the test set"
      ],
      "metadata": {
        "id": "rXSOrejsdjpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create an example directory\n",
        "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
        "foodvision_mini_examples_path.mkdir(parents=True,\n",
        "                                    exist_ok=True)\n",
        "\n",
        "# Collect three random test dataset image paths\n",
        "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
        "\n",
        "# Copy the three images to the examples directory\n",
        "for example in foodvision_mini_examples:\n",
        "  destination = foodvision_mini_examples_path / example.name\n",
        "  print(f\"[INFO] Copying {example} to {destination}\")\n",
        "  shutil.copy2(src=example,\n",
        "               dst=destination)"
      ],
      "metadata": {
        "id": "kpsrGudWfLGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gradio takes in a format of a list of list, so lets verify that we can get a list of list from our `examples/`directory"
      ],
      "metadata": {
        "id": "U9hbO7fchSdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_list"
      ],
      "metadata": {
        "id": "HnDnpie0fq6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get example file paths in a list of lists\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "98vOYsCHhQaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Moving our trained EffnetB2 model to our Foodvision Mini demo directory"
      ],
      "metadata": {
        "id": "g4r4gpgpiCZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# create a source path for our target model\n",
        "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
        "\n",
        "# Create a destination path for our target model\n",
        "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
        "\n",
        "# Try to move the model file\n",
        "try:\n",
        "  print(f\"[INFO] attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
        "\n",
        "  # Move the model\n",
        "  shutil.move(src=effnetb2_foodvision_mini_model_path,\n",
        "              dst=effnetb2_foodvision_mini_model_destination)\n",
        "\n",
        "  print(f\"[INFO] Model move complete.\")\n",
        "\n",
        "# If the model has already been moved, check if it exists\n",
        "except:\n",
        "  print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved\")\n",
        "  print(f\"[INFO] model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uC4UK7q8k4wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6 Turning off EffNetB2 model into a python script (`model.py`)\n",
        "\n",
        "we have a saved `.pth` model `stat_dict` and want to load it into a model instance.\n",
        "\n",
        "Let's move our `create_effnetb2_model()` function to a script so we can reuse it"
      ],
      "metadata": {
        "id": "6j9KhYZck48K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/model.py\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3, # Default put classes: [pizza, steak, sushi]\n",
        "                          seed:int=42):\n",
        "  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "  transforms = weights.transforms()\n",
        "\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # 4. Freeze all the base layers\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # 5. change classifier head with random seed for reproducibilty\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.3, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "id": "V6d_R4dAoXMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from demos.foodvision_mini import model\n",
        "\n",
        "effnetb2_model, effnetb2_transforms_import = model.create_effnetb2_model()\n",
        "effnetb2_model"
      ],
      "metadata": {
        "id": "KZOl-3_6oXP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.7 Turning our Foodvision Mini Gradio app into a Python script (`app.py`)\n",
        "\n",
        "The `app.py` file will have four major parts:\n",
        "1. Imports and class names setup\n",
        "2. Model and transforms preparation\n",
        "3. Predict function (`predict()`)\n",
        "4. Gradio app - our Gradio interface + launch command"
      ],
      "metadata": {
        "id": "yqSGgJpyoXfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/app.py\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "class_names = ['pizza', 'steak', 'sushi']\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=len(class_names)\n",
        ")\n",
        "\n",
        "# Load the saved weights\n",
        "effnetb2.load_state_dict(torch.load(\n",
        "    f=\"foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
        "    map_location=torch.device(\"cpu\") # load the model to the cpu\n",
        "))\n",
        "\n",
        "### 3. Predict function ###\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  # Start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Transform the input image for use with EffNetB2\n",
        "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on the 0th dimension\n",
        "\n",
        "  # Put model into eval mode, make prediction\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass transformed image through the model and turn the prediction logits into probabilities\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # Calculate the pred time\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description, and article\n",
        "\n",
        "title = \"FoodVision Mini🍕🥩🍥\"\n",
        "description = \"An [EfficientNetB2 feature extractor](https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2) computer vision model to classify images as pizza, steak, or sushi\"\n",
        "article = \"Created at [09. Pytorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/#1-getting-data)\"\n",
        "\n",
        "# Create example list\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create the gradio demo\n",
        "demo = gr.Interface(fn=predict, # Maps our inputs to outputs\n",
        "                    inputs=gr.Image(type=\"pil\"),\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"predictions\"),\n",
        "                             gr.Number(label=\"Prediction time (s)\")],\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL\n"
      ],
      "metadata": {
        "id": "ruW7G-3ipxV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.8 Creating a requirements file for foodvision mini (`requirements.txt`)\n",
        "\n",
        "The requirements file will tell our hugging face space what software dependencies our app requires.\n",
        "\n",
        "The three main ones are:\n",
        "* `torch`\n",
        "* `torhvision`\n",
        "* `gradio`"
      ],
      "metadata": {
        "id": "nj_Wj_kirJK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/requirements.txt\n",
        "torch==2.2.1\n",
        "torchvision==0.17.1\n",
        "gradio==4.22.0"
      ],
      "metadata": {
        "id": "17oJKO3xpxah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "id": "9t-R5kb1vm0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision.__version__"
      ],
      "metadata": {
        "id": "0QQXkdq-pxgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.__version__"
      ],
      "metadata": {
        "id": "wGo8iai0pxiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Deploying our Foodvision mini app HuggingFace Spaces\n",
        "here are two main options for uploading to a Hugging Face Space (also called a Hugging Face Repository, similar to a git repository):\n",
        "\n",
        "* Uploading via the Hugging Face Web interface (easiest).\n",
        "* ploading via the command line or terminal.\n",
        " * Bonus: You can also use the huggingface_hub library to interact with Hugging Face, this would be a good extension to the above two options.\n"
      ],
      "metadata": {
        "id": "PIDFpQk-vxT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 downloading our foodvision mini app files\n",
        "\n",
        "we want to download our `foodvision_mini` demo app so we can upload it to Hugging face spaces."
      ],
      "metadata": {
        "id": "C_W6wfPkxPDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls demos/foodvision_mini/examples"
      ],
      "metadata": {
        "id": "L4s2_EFSxPRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into the foodvision_mini directory and then zip it from the inside\n",
        "\n",
        "!cd demos/foodvision_mini && zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoint*\" # '-x' means exclude"
      ],
      "metadata": {
        "id": "BlzmwqbNyxGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd # print working directory"
      ],
      "metadata": {
        "id": "cn8li61izbkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download(\"demos/foodvision_mini.zip\")\n",
        "except:\n",
        "  print(f\"Not running in Google Colab, cant use google.colab.files.download() pleace download foodvision_mini.zip manually\")"
      ],
      "metadata": {
        "id": "u0hJm51Dzcmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Running our Gradio demo app locally\n",
        "running the app locally: https://www.learnpytorch.io/09_pytorch_model_deployment/#92-running-our-foodvision-mini-demo-locally"
      ],
      "metadata": {
        "id": "vivMufX10wJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3 Uploading our foodvision Mini gradio demo to hugging face spaces"
      ],
      "metadata": {
        "id": "FfBndeVUjK-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Creating Foodvision BIG!\n",
        "\n",
        "Foodvision Mini works well with 3 classes (pizza, steak, sushi)\n",
        "\n",
        "So all of experimenting is paying off..\n",
        "\n",
        "Let's Step things up a notch and make Foodvision BIG!! using all of the Food101 classes"
      ],
      "metadata": {
        "id": "wCyaCvt_jLCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Food101 model and transforms\n",
        "effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)"
      ],
      "metadata": {
        "id": "aoeLkEt7jLGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print EffNetB2 model summary (uncomment for full output)\n",
        "summary(effnetb2_food101,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "CdqPORRtjLKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_transforms"
      ],
      "metadata": {
        "id": "Ej3u8piUjLNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are working with a larger dataset, we may want to introduce some data augmentation techniques:\n",
        "* This is because with larger datasets and larger models, overfitting becomes more of a problem.\n",
        "* Because we are working with a large number of classes, lets use TrivialAugment as our data augmentation technique."
      ],
      "metadata": {
        "id": "RsnOrqWvQ-TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training data transforms\n",
        "food_101_trasnforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.TrivialAugmentWide(),\n",
        "    effnetb2_transforms\n",
        "])\n",
        "\n",
        "food_101_trasnforms"
      ],
      "metadata": {
        "id": "2PfcT4LWRySC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing data transforms, we dont want to augment our testing data\n",
        "effnetb2_transforms"
      ],
      "metadata": {
        "id": "0tMhHXskS-Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Getting data for FoodVision Big"
      ],
      "metadata": {
        "id": "vab2LwlpRyVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "# Setup data directory\n",
        "from pathlib import Path\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "# Get the training data (~750 x 101 classes)\n",
        "train_data = datasets.Food101(root=data_dir,\n",
        "                              split=\"train\",\n",
        "                              transform=food_101_trasnforms, # apply data augmentation to training data\n",
        "                              download=True)# Get the training data (~750 x 101 classes)\n",
        "\n",
        "# Get the testing data (~250 x 101 classes)\n",
        "test_data = datasets.Food101(root=data_dir,\n",
        "                              split=\"test\",\n",
        "                              transform=effnetb2_transforms, # apply data augmentation to testing data\n",
        "                              download=True)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "NI5gX3NJTdWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(750 * 101) + (250 * 101)"
      ],
      "metadata": {
        "id": "8HHYA25_T4vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get Food101 class names\n",
        "food101_class_names = train_data.classes\n",
        "\n",
        "# View the first 10\n",
        "food101_class_names[:10]"
      ],
      "metadata": {
        "id": "5uH2bI8gRyZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Creating a subset of the Food101 dataset for faster experimenting\n",
        "\n",
        "Why create a subset?\n",
        "\n",
        "We want our first few experiments to run as quick as possible\n",
        "\n",
        "We know Foodvision Mini works pretty well but thi is the first we've upgraded 101 classes\n",
        "\n",
        "To do so, let's make a subset of 20% of the data from the Food101 dataset (training and test).\n",
        "\n",
        "our short term goal: to beat the original Food101 paper results of 56.40% accuracy on the test dataset (see paper: https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf)\n",
        "we want to beat this result using modern deep leaning techniques and only 20% of the data"
      ],
      "metadata": {
        "id": "crjcm6JPRydj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "def split_dataset(dataset:torchvision.datasets,\n",
        "                  split_size:float=0.2,\n",
        "                  seed:int=42):\n",
        "  # Create split lengths based on original dataset length\n",
        "  length_1 = int(len(dataset) * split_size)\n",
        "  length_2 = len(dataset) - length_1\n",
        "\n",
        "  # print out info\n",
        "  print(f\"[INFO] splitting dataset off length {len(dataset)} into splits of size: {length_1} and {length_2}\")\n",
        "\n",
        "  # Create splits with given random seeds\n",
        "  random_split_1, random_split_2 = random_split(dataset,\n",
        "                                                lengths=[length_1, length_2],\n",
        "                                                generator=torch.manual_seed(seed))\n",
        "\n",
        "  return random_split_1, random_split_2"
      ],
      "metadata": {
        "id": "g2n9fIILYcK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training 20% split Food101\n",
        "train_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n",
        "                                                 split_size=0.2)\n",
        "\n",
        "test_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n",
        "                                                 split_size=0.2)"
      ],
      "metadata": {
        "id": "ChJnC1oEaR0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data_food101_20_percent), len(test_data_food101_20_percent)"
      ],
      "metadata": {
        "id": "MAJCKx5raR_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create Food101 20 percent training Dataloader\n",
        "train_dataloader_food101_20_percent = torch.utils.data.DataLoader(dataset=train_data_food101_20_percent,\n",
        "                                                                  batch_size=BATCH_SIZE,\n",
        "                                                                  shuffle=True,\n",
        "                                                                  num_workers=os.cpu_count())\n",
        "\n",
        "# Create Food101 20 percent testing dataloader\n",
        "test_dataloader_food101_20_percent = torch.utils.data.DataLoader(dataset=test_data_food101_20_percent,\n",
        "                                                                  batch_size=BATCH_SIZE,\n",
        "                                                                  shuffle=False,\n",
        "                                                                  num_workers=os.cpu_count())\n",
        "\n",
        "len(train_dataloader_food101_20_percent), len(test_dataloader_food101_20_percent)"
      ],
      "metadata": {
        "id": "XN5beBb9aSRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5 Training Foodvision Big!!!\n",
        "\n",
        "Things for training:\n",
        "* 5 epochs\n",
        "* Optimizer `torch.optim.Adam(lr=le-3)`\n",
        "* Loss function: `torch.nn.CrossEntropyLoss(label_smoothing=0.1)`\n",
        "\n",
        "Why use label smoothing?\n",
        "Label smoothing helps to prevent overfitting (it's a regularization technique).\n",
        "\n",
        "Without label smoothing and 5 classes:\n",
        "\n",
        "```\n",
        "[0.00, 0.00, 0.99, 0.01, 0.00]\n",
        "```\n",
        "With label smoothing and 5 classes:\n",
        "\n",
        "```\n",
        "[0.01, 0.01, 0.96, 0.01, 0.01]\n",
        "```\n"
      ],
      "metadata": {
        "id": "Xp5CTpIGaSe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup the optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "# Setup the loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# Want to beat the original Food101 papers results of 56.4% on the test dataset with 20% of the data\n",
        "set_seeds()\n",
        "\n",
        "# effnetb2_food101_results = engine.train(model=effnetb2_food101,\n",
        "#                                         train_dataloader=train_dataloader_food101_20_percent,\n",
        "#                                         test_dataloader=test_dataloader_food101_20_percent,\n",
        "#                                         optimizer=optimizer,\n",
        "#                                         loss_fn=loss_fn,\n",
        "#                                         epochs=5,\n",
        "#                                         device=device)"
      ],
      "metadata": {
        "id": "U2oC3sQEaSma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.6 inspecting loss curves of FoodVision Big model"
      ],
      "metadata": {
        "id": "LYSbzctNiLez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(effnetb2_food101_results)"
      ],
      "metadata": {
        "id": "MQXXuDsNocqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7 Save and load Foodvision Big Model"
      ],
      "metadata": {
        "id": "D3zi_RaAowbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# create model path\n",
        "effnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_20_percent.pth\"\n",
        "\n",
        "# Save the foodvision big model\n",
        "utils.save_model(model=effnetb2_food101,\n",
        "                 target_dir=\"models/\",\n",
        "                 model_name=effnetb2_food101_model_path)"
      ],
      "metadata": {
        "id": "wFdBUsHuo2Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Food101 compatible EffNetB2 instance\n",
        "loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n",
        "\n",
        "# Load the save models state_dict()\n",
        "loaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_20_percent.pth\"))"
      ],
      "metadata": {
        "id": "WlNLKnVapeX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.8 Checking the FoodVision Big model size"
      ],
      "metadata": {
        "id": "tT_zTCHLiLkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024 * 1024)\n",
        "print(f\"Pretrained effnetb2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")"
      ],
      "metadata": {
        "id": "iEQk5JSKiLqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Turning our Foodvision Big model into a deployable app\n",
        "\n",
        "Why deploy a model?\n",
        "\n",
        "Deploying a model allows you to see how your model goes in the real-world (the ultimate test set)\n",
        "\n",
        "Let's Create an outline for our FoodVision Big app:\n",
        "\n",
        "```\n",
        "demos/\n",
        "  foodvision_big/\n",
        "    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n",
        "    app.py\n",
        "    class_names.txt\n",
        "    examples/\n",
        "      example_1.jpg\n",
        "    model.py\n",
        "    requirements.txt\n",
        "```\n"
      ],
      "metadata": {
        "id": "GVeSdQ-IaSsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create Foodvision Big demo path\n",
        "foodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n",
        "\n",
        "# Make Foodvision Big demo directory\n",
        "foodvision_big_demo_path.mkdir(parents=True,\n",
        "                               exist_ok=True)\n",
        "\n",
        "# Make Foodvision Big demo examples directory\n",
        "(foodvision_big_demo_path / \"examples\").mkdir(parents=True,\n",
        "                                              exist_ok=True)\n"
      ],
      "metadata": {
        "id": "X9eqhSniCrpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls demos/foodvision_big # shows what sub directories are in there"
      ],
      "metadata": {
        "id": "KQzg7_oFCrtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.1 Download an example image and moving it to the `examples` directory"
      ],
      "metadata": {
        "id": "sLiEOjaCCrx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"data/food-101/images/apple_pie/1043283.jpg\""
      ],
      "metadata": {
        "id": "egvz4mxjF9qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv models/09_pretrained_effnetb2_feature_extractor_20_percent.pth demos/foodvision_big # moves the model to the foodvision_big directory"
      ],
      "metadata": {
        "id": "knmCkyCwF9tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2 Saving Food101 class names to file (class_names.txt)\n",
        "\n",
        "Let's save all of the food101 class names to a .txt file so we can import them and use them in our app."
      ],
      "metadata": {
        "id": "9x0Dbb5eKH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "food101_class_names = train_data.classes"
      ],
      "metadata": {
        "id": "3G2b_gv9F9xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food101_class_names[:10]"
      ],
      "metadata": {
        "id": "BI1hsE3NJ0ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a path to Food101 class names\n",
        "foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n",
        "\n",
        "foodvision_big_class_names_path"
      ],
      "metadata": {
        "id": "JrPjCUaILhXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Food101 class names to text file\n",
        "with open(foodvision_big_class_names_path, \"w\") as f:\n",
        "  print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n",
        "  f.write(\"\\n\".join(food101_class_names)) # New line per class name"
      ],
      "metadata": {
        "id": "88LfVQSELvyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open Food101 class names file and read each line into a list\n",
        "with open(foodvision_big_class_names_path, \"r\") as f:\n",
        "  food101_class_names_loaded = [food.strip() for food in f.readlines()]\n",
        "\n",
        "food101_class_names_loaded[:5]"
      ],
      "metadata": {
        "id": "Ee_BYsSPMWmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3 Turning our Foodvision Big model into a python script (`model.py`)\n"
      ],
      "metadata": {
        "id": "s1WuiTowNR3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/model.py\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=101, # Default put classes: [pizza, steak, sushi]\n",
        "                          seed:int=42):\n",
        "  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "  transforms = weights.transforms()\n",
        "\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # 4. Freeze all the base layers\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # 5. change classifier head with random seed for reproducibilty\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.3, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "id": "YecvH3VwOUQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4 Turning our Foodvision Big Gradio app into a Python script (`app.py`)\n",
        "\n",
        "The `app.py` file will have four major parts:\n",
        "1. Imports and class names setup - for class names,  we'll need to import from `class_names.txt`\n",
        "2. Model and transforms preparation - we'll need to make sure our model is suitable for Foodvision Big\n",
        "3. Predict function (`predict()`) - This can stay the same as the original `predict()`\n",
        "4. Gradio app - our Gradio interface + launch command - this will change slightly from Foodvision Mini to reflect the Foodvision big updates"
      ],
      "metadata": {
        "id": "BfrPsqBZOUaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/app.py\n",
        "\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Set up the class names\n",
        "with open(\"class_names.txt\", \"r\") as f:\n",
        "  class_names = [food.strip() for food in f.readlines()]\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "# Create model and transforms\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n",
        "\n",
        "# Load the saved weights\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(f=\"09_pretrained_effnetb2_feature_extractor_20_percent.pth\",\n",
        "                                    map_location=torch.device(\"cpu\")) # Load it to the cpu\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  # Start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Transform the input image for use with EffNetB2\n",
        "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on the 0th dimension\n",
        "\n",
        "  # Put model into eval mode, make prediction\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass transformed image through the model and turn the prediction logits into probabilities\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # Calculate the pred time\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio App ###\n",
        "# Create title, description, and article\n",
        "\n",
        "title = \"FoodVision BIG 🍔👁️💪🏽\"\n",
        "description = \"An [EfficientNetB2 feature extractor](https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2) computer vision model to classify images (101 classes of food from the Food101 dataset)\"\n",
        "article = \"Created at [09. Pytorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/#11-turning-our-foodvision-big-model-into-a-deployable-app)\"\n",
        "\n",
        "# Create example list\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create the gradio demo\n",
        "demo = gr.Interface(fn=predict, # Maps our inputs to outputs\n",
        "                    inputs=gr.Image(type=\"pil\"),\n",
        "                    outputs=[gr.Label(num_top_classes=5, label=\"predictions\"),\n",
        "                             gr.Number(label=\"Prediction time (s)\")],\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL"
      ],
      "metadata": {
        "id": "zQY52ILtPI1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5 Creating a requirements file for Foodvision Big (`requirements.txt`)"
      ],
      "metadata": {
        "id": "2C5SENx3T02i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/requirements.txt\n",
        "torch==2.2.1\n",
        "torchvision==0.17.1\n",
        "gradio==4.22.0\n"
      ],
      "metadata": {
        "id": "wwfOCE8mPJD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.6 Downloading our Foodvision big app files"
      ],
      "metadata": {
        "id": "CIJ6Jf9qTPuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into the foodvision_mini directory and then zip it from the inside\n",
        "\n",
        "!cd demos/foodvision_big && zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoint*\" # '-x' means exclude"
      ],
      "metadata": {
        "id": "_t9QizPsTP59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download(\"demos/foodvision_big.zip\")\n",
        "except:\n",
        "  print(f\"Not running in Google Colab, cant use google.colab.files.download() pleace download foodvision_big.zip manually\")"
      ],
      "metadata": {
        "id": "W6J2MRMIPHmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.7 Deploying our Foodvision Big model app to HuggingFace spaces\n",
        "\n",
        "Let's bring foodvision Big to life by deploying it to the world\n",
        "https://huggingface.co/spaces/burhanji1/foodvision_big_1"
      ],
      "metadata": {
        "id": "NDwPAxR8WB3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises and Extra curriculum"
      ],
      "metadata": {
        "id": "cvi8zaufMoZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1. Make and time predictions with both feature extractor models on the test dataset using the GPU (device=\"cuda\")."
      ],
      "metadata": {
        "id": "Sv7_maPTPdf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2. The ViT feature extractor seems to have more learning capacity (due to more parameters) than EffNetB2, how does it go on the larger 20% split of the entire Food101 dataset?\n",
        "\n",
        "Train a ViT feature extractor on the 20% Food101 dataset for 5 epochs, just like we did with EffNetB2 in section 10. Creating FoodVision Big"
      ],
      "metadata": {
        "id": "LlOq-OwePdsk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCiXpOpAPdw-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}